{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Intensity based Object Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for object tracking\n",
    "if not os.path.isdir(os.path.join(os.getcwd(), 'frames')):\n",
    "    os.mkdir(\"frames\")\n",
    "else:\n",
    "    print('frames already exists')\n",
    "\n",
    "if not os.path.isdir(os.path.join(os.getcwd(), 'composite')):\n",
    "    os.mkdir(\"composite\")\n",
    "else:\n",
    "    print('composite already exists')\n",
    "    \n",
    "framenumber = 0\n",
    "framectr = 0\n",
    "omovie = cv2.VideoCapture('ping_pang.mov')\n",
    "frame_height = omovie.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "frame_width  = omovie.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "\n",
    "# Extract the frames from original video\n",
    "while(1):\n",
    "    ret, frame = omovie.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    print('Extracting: %d' % framenumber)\n",
    "    clear_output(wait=True)\n",
    "    cv2.imwrite('frames/%d.tif' % framenumber, frame)\n",
    "    framenumber += 1\n",
    "omovie.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Find the object coordinate by averaging the foreground coordinates\n",
    "    - Function Input: \n",
    "            -                frame: numpy array, the frame to be processed\n",
    "    - Function Output:\n",
    "            - [object_x, object_y]: the coordinate of the object in the frame\n",
    "'''\n",
    "def findObj(frame):\n",
    "    ############# TO DO #############\n",
    "    \n",
    "    \n",
    "    \n",
    "    #############  END  #############\n",
    "    return [object_x, object_y]\n",
    "\n",
    "\n",
    "# Draw a circle on the image.\n",
    "def drawbox(frame, centerx, centery, radius, color):\n",
    "    for y in range(centery - radius, centery + radius):\n",
    "        for x in range(centerx - radius, centerx + radius):\n",
    "            cy = 0 if y < 0 else frame.shape[0] - 1 if y > frame.shape[0] - 1 else y\n",
    "            cx = 0 if x < 0 else frame.shape[1] - 1 if x > frame.shape[1] - 1 else x\n",
    "            for i in range(3):\n",
    "                frame[cy][cx][i] = color[i]\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation Tips:\n",
    "* You can use `np.hstack((img1, img2))` to horizontally stack two images into one\n",
    "* You can use `cv2.putText(combined_img, text='Sample Text', org=(2200, 1000), fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=5, thickness = 5, color=(0, 255, 0)` to put text on a frame\n",
    "* You can use `cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 2)` to draw a red line between two points (x1, y1) and (x2, y2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "framectr = framenumber - 1\n",
    "process_frame = 0\n",
    "\n",
    "foreground_tvalue = 250 # Foreground Threshold-Value (tvalue) for Segmentation\n",
    "# Store the coordinates found by intensity thresholding\n",
    "coordListX = list()\n",
    "coordListY = list()\n",
    "\n",
    "while process_frame <= framectr:\n",
    "    # Only keep a small range of frames to save the processing time, i.e., frames within (53, 94).\n",
    "    if process_frame > 94:\n",
    "        print(\"Stop processing frames to save time\")\n",
    "        break\n",
    "    if process_frame < 53:\n",
    "        process_frame += 1\n",
    "        continue\n",
    "    \n",
    "    # Read RGB frame\n",
    "    rgb_frame = cv2.imread('frames/%d.tif' % process_frame) \n",
    "    print('Processing frame: %d, overall progress: %.2f %%' % (process_frame, process_frame/framectr*100))\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Object Segmentation \n",
    "    # 1. convert rgb_frame to gray scale, \n",
    "    # 2. then perform forground segmentation (based on its grayscale values).\n",
    "    g_frame = rgb_frame.copy() # g_frame for Grayscaled frame \n",
    "    for y in range(gframe.shape[1]):\n",
    "        for x in range(gframe.shape[0]):\n",
    "            # 1. Convert to gray scale\n",
    "            g = 0.212671 * gframe[x][y][2] + 0.715160 * gframe[x][y][1] + 0.072169 * gframe[x][y][0]\n",
    "            \n",
    "            # 2. Perform forground segmentation to generate a binary mask \n",
    "            #   (0  -black-background)\n",
    "            #   (255-white-foreground)\n",
    "            for i in range(3):\n",
    "                gframe[x][y][i] = 255 if g > foreground_tvalue else 0\n",
    "    \n",
    "    # Theoretically, g_frame should only contain 1 color-channel, i.e., (H*W*1)\n",
    "    # However, here, we now set g_frame with 3 color-channel on purpose, i.e., (H*W*3)\n",
    "    #        - as np.hstack() below expects g_frame and rgb_frame to share the same size.\n",
    "    # So, you could simply treat g_frame as a \"fake-rgb\" frame (with 3 channels), \n",
    "    #        - and all three channels contain exactly the same information (grayscale image).\n",
    "    \n",
    "    # Get the initial state (object coordinates) from binary segmentation\n",
    "    coord = findObj(gframe) # coord is the centre of mass\n",
    "            \n",
    "    # Draw a red dot in the centre of the segmented object\n",
    "    rgb_frame = drawbox(rgb_frame, int(coord[0]), int(coord[1]), 5, (0, 0, 255))\n",
    "    gframe    = drawbox(gframe   , int(coord[0]), int(coord[1]), 5, (0, 0, 255))\n",
    "    combined_img = np.hstack((rgb_frame, gframe))\n",
    "    \n",
    "    ####################################### TODO ###############################################\n",
    "    # 1. You need to perform the object segmentation results on 'gframe'                       #\n",
    "    # 2. You need to display the object moving trajectory on 'rgb_frame'                       #\n",
    "    # 3. You need to display a text to determine whether the object is found or not in a frame #\n",
    "    # 4. Please see the lab sheet for reference                                                #\n",
    "    \n",
    "    \n",
    "\n",
    "    ####################################### END ################################################\n",
    "    \n",
    "    cv2.imwrite('composite/composite%d.tif' % process_frame, combined_img)\n",
    "    if cv2.waitKey(30) & 0xff == ord('q'):\n",
    "        break\n",
    "    process_frame += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "out = cv2.VideoWriter('./happy_ping_pang.mov', cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 10, (int(frame_width*2), int(frame_height)))\n",
    "while(1):\n",
    "    img = cv2.imread('composite/composite%d.tif' % count)\n",
    "    if img is None:\n",
    "        print('No more frames to be loaded')\n",
    "        break\n",
    "    clear_output(wait=True)\n",
    "    out.write(img)\n",
    "    count += 1\n",
    "    print('Saving video: %d%%' % int(100*count/framenumber))\n",
    "    \n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity based Object Tracking - Week5 Lab Checkpoint Submission\n",
    "You can now use the generate_results() function below to generate your outputs for submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    generate_results function is a helper function for you to generate\n",
    "    the output images of lab exercise submission\n",
    "    - Function Input: \n",
    "            -            wk:           int, indicates a specific week's lab exercise\n",
    "            -          name:           str, the name of the student\n",
    "            -           SID:           int, the SID of the student\n",
    "            -  output_video:           str, the path to output_video\n",
    "\n",
    "    - Function Usage:\n",
    "            - Supply all the arguments with the correct types and a result image\n",
    "              will be generated.\n",
    "    - Tips:\n",
    "            - You can right click the result image plot to save the image or \n",
    "              you can take a screenshoot for the submission.\n",
    "'''\n",
    "def generate_results(wk, name, SID, output_video):\n",
    "    cap = cv2.VideoCapture(output_video)\n",
    "    random_frames = []\n",
    "    if not cap.isOpened():\n",
    "        print('%s not opened' % output_video.split('/')[-1])\n",
    "        sys.exit(1)\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    x = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    y = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    while x > 10:\n",
    "        x /= (x / 10)\n",
    "        y /= (y / 10)\n",
    "    \n",
    "    random_frames.append(random.randint(53, 63))\n",
    "    random_frames.append(random.randint(66, 76))\n",
    "    random_frames.append(random.randint(80, 94))\n",
    "        \n",
    "    fig, axs = plt.subplots(3, 1, figsize=(x,y))\n",
    "        \n",
    "    count = 0\n",
    "    output_count = 0\n",
    "    while(1):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count in random_frames:\n",
    "            frame_m = frame.copy()\n",
    "            frame_R = frame[:,:,2]\n",
    "            frame_B = frame[:,:,0]\n",
    "            frame_m[:,:,2] = frame_B\n",
    "            frame_m[:,:,0] = frame_R\n",
    "            frame = np.uint8(frame_m)\n",
    "\n",
    "            axs[output_count].imshow(frame_m)\n",
    "            axs[output_count].text(0.5, -0.1, 'Composite frame: ' + str(count), size=12, ha=\"center\", transform=axs[output_count].transAxes)\n",
    "            axs[output_count].axis('off')\n",
    "            output_count+=1\n",
    "            \n",
    "            if output_count >= 3:\n",
    "                break\n",
    "        count+=1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    fig.suptitle(\"Week %i Lab Exercise\\n %s SID:%i\"%(wk, name, SID),x=0.5,y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the 'path_to_output' to the path where your composited video is located\n",
    "path_to_output = './happy_ping_pang.mov'\n",
    "generate_results(5, 'Your name', 12345567, path_to_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
